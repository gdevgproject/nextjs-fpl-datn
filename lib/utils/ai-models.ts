/**
 * AI model utility ƒë·ªÉ qu·∫£n l√Ω vi·ªác chuy·ªÉn ƒë·ªïi gi·ªØa c√°c model khi g·∫∑p rate limit
 *
 * C√°ch s·ª≠ d·ª•ng:
 * ```
 * import { getGroqModel, handleModelRateLimit } from '@/lib/utils/ai-models';
 *
 * // L·∫•y model hi·ªán t·∫°i ƒë·ªÉ s·ª≠ d·ª•ng
 * const model = getGroqModel();
 *
 * try {
 *   // G·ªçi API v·ªõi model n√†y
 *   const response = await callGroqAPI(model, ...);
 * } catch (error) {
 *   // X·ª≠ l√Ω rate limit v√† l·∫•y model thay th·∫ø n·∫øu c·∫ßn
 *   if (isRateLimitError(error)) {
 *     const alternativeModel = handleModelRateLimit(model);
 *     // Th·ª≠ l·∫°i v·ªõi model thay th·∫ø
 *     const response = await callGroqAPI(alternativeModel, ...);
 *   }
 * }
 * ```
 */

// Danh s√°ch model theo th·ª© t·ª± ∆∞u ti√™n
const DEFAULT_MODEL_PRIORITY = [
  "openai/gpt-oss-120b",
  "openai/gpt-oss-20b",
  "meta-llama/llama-4-maverick-17b-128e-instruct", // ∆Øu ti√™n 1
  "meta-llama/llama-4-scout-17b-16e-instruct", // ∆Øu ti√™n 2
  "llama-3.3-70b-versatile", // ∆Øu ti√™n 3
  "llama-3.1-8b-instant", // ∆Øu ti√™n 4
  "deepseek-r1-distill-llama-70b", // ∆Øu ti√™n 5
  "llama3-70b-8192", // ∆Øu ti√™n 6
  "llama3-8b-8192", // ∆Øu ti√™n 7
  "gemma2-9b-it", // ∆Øu ti√™n 8 (cu·ªëi)
];

// L·∫•y danh s√°ch model t·ª´ m√¥i tr∆∞·ªùng ho·∫∑c s·ª≠ d·ª•ng danh s√°ch m·∫∑c ƒë·ªãnh
const MODEL_PRIORITY = process.env.GROQ_MODEL_PRIORITY
  ? process.env.GROQ_MODEL_PRIORITY.split(",").map((model) => model.trim())
  : DEFAULT_MODEL_PRIORITY;

// L∆∞u tr·∫°ng th√°i rate limit c·ªßa c√°c model
const modelRateLimits: Record<
  string,
  {
    limited: boolean;
    retryAt: number;
    retryCount: number;
  }
> = {};

// Kh·ªüi t·∫°o tr·∫°ng th√°i cho t·∫•t c·∫£ model
MODEL_PRIORITY.forEach((model) => {
  modelRateLimits[model] = { limited: false, retryAt: 0, retryCount: 0 };
});

// C√°c h·∫±ng s·ªë c·∫•u h√¨nh
const RATE_LIMIT_RESET_TIME = 60 * 1000; // 1 ph√∫t
const MAX_RETRY_COUNT = 5; // S·ªë l·∫ßn th·ª≠ t·ªëi ƒëa tr∆∞·ªõc khi ng·ª´ng t·ª± ƒë·ªông reset
const MAX_FALLBACK_RETRIES = 2; // S·ªë l·∫ßn th·ª≠ t·ªëi ƒëa cho model thay th·∫ø tr∆∞·ªõc khi th·ª≠ model ti·∫øp theo

/**
 * Ki·ªÉm tra xem l·ªói c√≥ ph·∫£i l√† do rate limit kh√¥ng
 */
export function isRateLimitError(error: any): boolean {
  // Ki·ªÉm tra th√¥ng b√°o l·ªói c·ªßa Groq API cho rate limit
  if (typeof error === "string") {
    return error.includes("rate_limit_exceeded");
  }

  // Ki·ªÉm tra ƒë·ªëi t∆∞·ª£ng l·ªói
  if (
    error?.error?.code === "rate_limit_exceeded" ||
    (error?.message && error.message.includes("rate_limit_exceeded"))
  ) {
    return true;
  }

  // Ki·ªÉm tra response t·ª´ fetch API
  if (error?.status === 429) {
    return true;
  }

  return false;
}

/**
 * ƒê√°nh d·∫•u model ƒë√£ g·∫∑p rate limit v√† l·∫•y model thay th·∫ø
 */
export function handleModelRateLimit(currentModel: string): string {
  // ƒê√°nh d·∫•u model hi·ªán t·∫°i l√† b·ªã rate limit
  if (modelRateLimits[currentModel]) {
    modelRateLimits[currentModel].limited = true;
    modelRateLimits[currentModel].retryAt = Date.now() + RATE_LIMIT_RESET_TIME;
    modelRateLimits[currentModel].retryCount += 1;

    console.log(
      `[AI-MODEL] üî¥ Model ${currentModel} b·ªã rate limit. Chuy·ªÉn sang model thay th·∫ø.`
    );
  }

  // T·ª± ƒë·ªông reset rate limit sau m·ªôt th·ªùi gian n·∫øu s·ªë l·∫ßn th·ª≠ ch∆∞a v∆∞·ª£t qu√° ng∆∞·ª°ng
  if (modelRateLimits[currentModel]?.retryCount <= MAX_RETRY_COUNT) {
    setTimeout(() => {
      if (modelRateLimits[currentModel]) {
        modelRateLimits[currentModel].limited = false;
        console.log(
          `[AI-MODEL] üü¢ Reset rate limit cho model ${currentModel}.`
        );
      }
    }, RATE_LIMIT_RESET_TIME);
  }

  // Tr·∫£ v·ªÅ model thay th·∫ø theo th·ª© t·ª± ∆∞u ti√™n
  return getNextAvailableModel(currentModel);
}

/**
 * L·∫•y model ti·∫øp theo c√≥ s·∫µn theo th·ª© t·ª± ∆∞u ti√™n
 */
function getNextAvailableModel(currentModel: string): string {
  // T√¨m v·ªã tr√≠ c·ªßa model hi·ªán t·∫°i trong danh s√°ch ∆∞u ti√™n
  const currentIndex = MODEL_PRIORITY.indexOf(currentModel);

  // N·∫øu kh√¥ng t√¨m th·∫•y model hi·ªán t·∫°i trong danh s√°ch, tr·∫£ v·ªÅ model ƒë·∫ßu ti√™n
  if (currentIndex === -1) return MODEL_PRIORITY[0];

  // T√¨m model ti·∫øp theo ch∆∞a b·ªã rate limit
  for (let i = 0; i < MODEL_PRIORITY.length; i++) {
    // B·∫Øt ƒë·∫ßu t·ª´ model sau model hi·ªán t·∫°i, n·∫øu ƒë·∫øn cu·ªëi danh s√°ch th√¨ quay l·∫°i ƒë·∫ßu
    const nextIndex = (currentIndex + i + 1) % MODEL_PRIORITY.length;
    const nextModel = MODEL_PRIORITY[nextIndex];

    // N·∫øu model ti·∫øp theo kh√¥ng b·ªã rate limit, s·ª≠ d·ª•ng n√≥
    if (!modelRateLimits[nextModel]?.limited) {
      return nextModel;
    }
  }

  // N·∫øu t·∫•t c·∫£ model ƒë·ªÅu b·ªã rate limit, tr·∫£ v·ªÅ model c√≥ th·ªùi gian retry s·ªõm nh·∫•t
  return MODEL_PRIORITY.reduce((earliest, model) => {
    if (
      !modelRateLimits[earliest] ||
      (modelRateLimits[model] &&
        modelRateLimits[model].retryAt < modelRateLimits[earliest].retryAt)
    ) {
      return model;
    }
    return earliest;
  }, MODEL_PRIORITY[0]);
}

/**
 * L·∫•y model ph√π h·ª£p nh·∫•t ƒë·ªÉ s·ª≠ d·ª•ng t·∫°i th·ªùi ƒëi·ªÉm hi·ªán t·∫°i
 * ∆Øu ti√™n theo th·ª© t·ª± trong MODEL_PRIORITY
 */
export function getGroqModel(): string {
  // D·ªçn d·∫πp tr·∫°ng th√°i rate limit n·∫øu ƒë√£ h·∫øt th·ªùi gian
  Object.keys(modelRateLimits).forEach((model) => {
    if (
      modelRateLimits[model]?.limited &&
      Date.now() >= modelRateLimits[model].retryAt
    ) {
      modelRateLimits[model].limited = false;
      console.log(
        `[AI-MODEL] üü¢ T·ª± ƒë·ªông reset rate limit cho model ${model} do h·∫øt th·ªùi gian ch·ªù.`
      );
    }
  });

  // T√¨m model ƒë·∫ßu ti√™n trong danh s√°ch ∆∞u ti√™n m√† kh√¥ng b·ªã rate limit
  for (const model of MODEL_PRIORITY) {
    if (!modelRateLimits[model]?.limited) {
      return model;
    }
  }

  // N·∫øu t·∫•t c·∫£ model ƒë·ªÅu b·ªã rate limit, tr·∫£ v·ªÅ model c√≥ th·ªùi gian retry s·ªõm nh·∫•t
  return MODEL_PRIORITY.reduce((earliest, model) => {
    if (
      !modelRateLimits[earliest] ||
      (modelRateLimits[model] &&
        modelRateLimits[model].retryAt < modelRateLimits[earliest].retryAt)
    ) {
      return model;
    }
    return earliest;
  }, MODEL_PRIORITY[0]);
}

/**
 * H√†m ti·ªán √≠ch ƒë·ªÉ g·ªçi Groq API v·ªõi x·ª≠ l√Ω rate limit
 */
export async function callGroqAPI(
  messages: any[],
  options: {
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
  } = {}
): Promise<Response> {
  // L·∫•y model ph√π h·ª£p nh·∫•t hi·ªán t·∫°i
  let model = getGroqModel();
  const apiKey = process.env.GROQ_API_KEY;

  if (!apiKey) {
    throw new Error("GROQ_API_KEY is not defined");
  }

  // Log model ƒë∆∞·ª£c s·ª≠ d·ª•ng
  console.log(`[AI-MODEL] ü§ñ ƒêang s·ª≠ d·ª•ng model: ${model}`);

  try {
    // B·∫Øt ƒë·∫ßu th·ªùi gian request
    const startTime = Date.now();

    const response = await fetch(
      "https://api.groq.com/openai/v1/chat/completions",
      {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${apiKey}`,
        },
        body: JSON.stringify({
          model,
          messages,
          stream: options.stream ?? true,
          temperature: options.temperature ?? 0.7,
          max_tokens: options.max_tokens ?? 1024,
        }),
      }
    );

    // Th·ªùi gian ho√†n th√†nh request
    const requestTime = Date.now() - startTime;

    // N·∫øu API tr·∫£ v·ªÅ l·ªói
    if (!response.ok) {
      try {
        // Clone response tr∆∞·ªõc khi ƒë·ªçc body ƒë·ªÉ c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng
        const clonedResponse = response.clone();
        const responseText = await clonedResponse.text();
        let responseBody;

        try {
          responseBody = JSON.parse(responseText);
        } catch (e) {
          responseBody = { error: { message: responseText } };
        }

        // Ki·ªÉm tra xem c√≥ ph·∫£i l·ªói rate limit kh√¥ng
        if (
          response.status === 429 ||
          (responseBody?.error?.message &&
            responseBody.error.message.includes("rate_limit_exceeded"))
        ) {
          console.error(
            `[AI-MODEL] ‚ö†Ô∏è Rate limit reached for model: ${model}. Response: ${responseText}`
          );

          // X·ª≠ l√Ω rate limit v√† l·∫•y model thay th·∫ø
          let alternativeModel = model;
          const triedModels = new Set([model]); // L∆∞u l·∫°i c√°c model ƒë√£ th·ª≠ ƒë·ªÉ tr√°nh l·∫∑p v√¥ h·∫°n

          // Th·ª≠ t·ª´ng model theo th·ª© t·ª± ∆∞u ti√™n cho ƒë·∫øn khi t√¨m ƒë∆∞·ª£c model ho·∫°t ƒë·ªông
          for (
            let attempt = 0;
            attempt < MODEL_PRIORITY.length - 1;
            attempt++
          ) {
            // L·∫•y model thay th·∫ø ti·∫øp theo
            alternativeModel = handleModelRateLimit(alternativeModel);

            // N·∫øu ƒë√£ th·ª≠ model n√†y r·ªìi, b·ªè qua ƒë·ªÉ tr√°nh l·∫∑p v√¥ h·∫°n
            if (triedModels.has(alternativeModel)) continue;
            triedModels.add(alternativeModel);

            console.log(
              `[AI-MODEL] üîÄ Switching to alternative model: ${alternativeModel}`
            );

            // B·∫Øt ƒë·∫ßu th·ªùi gian request thay th·∫ø
            const altStartTime = Date.now();

            // Th·ª≠ v·ªõi model thay th·∫ø
            try {
              const alternativeResponse = await fetch(
                "https://api.groq.com/openai/v1/chat/completions",
                {
                  method: "POST",
                  headers: {
                    "Content-Type": "application/json",
                    Authorization: `Bearer ${apiKey}`,
                  },
                  body: JSON.stringify({
                    model: alternativeModel,
                    messages,
                    stream: options.stream ?? true,
                    temperature: options.temperature ?? 0.7,
                    max_tokens: options.max_tokens ?? 1024,
                  }),
                }
              );

              // Th·ªùi gian ho√†n th√†nh request thay th·∫ø
              const altRequestTime = Date.now() - altStartTime;

              if (alternativeResponse.ok) {
                console.log(
                  `[AI-MODEL] ‚úÖ Request v·ªõi model thay th·∫ø ${alternativeModel} th√†nh c√¥ng sau ${altRequestTime}ms`
                );
                return alternativeResponse;
              } else {
                const altErrorText = await alternativeResponse.text();
                console.error(
                  `[AI-MODEL] ‚ùå Request v·ªõi model thay th·∫ø ${alternativeModel} th·∫•t b·∫°i sau ${altRequestTime}ms: ${altErrorText}`
                );

                // Ki·ªÉm tra n·∫øu l·ªói l√† do rate limit, ƒë√°nh d·∫•u model n√†y c≈©ng b·ªã rate limit
                if (
                  alternativeResponse.status === 429 ||
                  altErrorText.includes("rate_limit_exceeded")
                ) {
                  handleModelRateLimit(alternativeModel);
                }
              }
            } catch (attemptError) {
              console.error(
                `[AI-MODEL] ‚ùå L·ªói k·∫øt n·ªëi khi th·ª≠ model ${alternativeModel}:`,
                attemptError
              );
            }
          }

          // N·∫øu ƒë√£ th·ª≠ t·∫•t c·∫£ model v√† kh√¥ng c√≥ model n√†o ho·∫°t ƒë·ªông
          console.error(
            `[AI-MODEL] ‚ùå ƒê√£ th·ª≠ t·∫•t c·∫£ ${triedModels.size} model nh∆∞ng kh√¥ng c√≥ model n√†o kh·∫£ d·ª•ng.`
          );

          // Tr·∫£ v·ªÅ response l·ªói c√≥ th·ªÉ x·ª≠ l√Ω
          return new Response(
            JSON.stringify({
              error: "T·∫•t c·∫£ model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng, vui l√≤ng th·ª≠ l·∫°i sau.",
              suggestions: [],
            }),
            {
              status: 503,
              headers: {
                "Content-Type": "application/json",
              },
            }
          );
        }

        // N·∫øu kh√¥ng ph·∫£i l·ªói rate limit, throw l·ªói
        console.error(
          `[AI-MODEL] ‚ùå Error from Groq API (model: ${model}): ${responseText}`
        );
        throw new Error(`Error from Groq API: ${responseText}`);
      } catch (parseError) {
        console.error(
          `[AI-MODEL] ‚ùå Error parsing API response: ${parseError}`
        );
        throw parseError;
      }
    }

    console.log(
      `[AI-MODEL] ‚úÖ Request th√†nh c√¥ng v·ªõi model ${model} sau ${requestTime}ms`
    );
    return response;
  } catch (error) {
    console.error(
      `[AI-MODEL] ‚ùå Error calling Groq API (model: ${model}):`,
      error
    );
    throw error;
  }
}
