/**
 * AI model utility ƒë·ªÉ qu·∫£n l√Ω vi·ªác chuy·ªÉn ƒë·ªïi gi·ªØa c√°c model khi g·∫∑p rate limit
 *
 * C√°ch s·ª≠ d·ª•ng:
 * ```
 * import { getGroqModel, handleModelRateLimit } from '@/lib/utils/ai-models';
 *
 * // L·∫•y model hi·ªán t·∫°i ƒë·ªÉ s·ª≠ d·ª•ng
 * const model = getGroqModel();
 *
 * try {
 *   // G·ªçi API v·ªõi model n√†y
 *   const response = await callGroqAPI(model, ...);
 * } catch (error) {
 *   // X·ª≠ l√Ω rate limit v√† l·∫•y model thay th·∫ø n·∫øu c·∫ßn
 *   if (isRateLimitError(error)) {
 *     const alternativeModel = handleModelRateLimit(model);
 *     // Th·ª≠ l·∫°i v·ªõi model thay th·∫ø
 *     const response = await callGroqAPI(alternativeModel, ...);
 *   }
 * }
 * ```
 */

// Danh s√°ch model ƒë∆∞·ª£c h·ªó tr·ª£
const MODELS = {
  PRIMARY:
    process.env.GROQ_PRIMARY_MODEL ||
    "meta-llama/llama-4-maverick-17b-128e-instruct",
  FALLBACK: process.env.GROQ_FALLBACK_MODEL || "llama-3.3-70b-versatile",
};

// L∆∞u tr·∫°ng th√°i rate limit c·ªßa c√°c model
const modelRateLimits: Record<
  string,
  {
    limited: boolean;
    retryAt: number;
    retryCount: number;
  }
> = {
  [MODELS.PRIMARY]: { limited: false, retryAt: 0, retryCount: 0 },
  [MODELS.FALLBACK]: { limited: false, retryAt: 0, retryCount: 0 },
};

// C√°c h·∫±ng s·ªë c·∫•u h√¨nh
const RATE_LIMIT_RESET_TIME = 60 * 1000; // 1 ph√∫t
const MAX_RETRY_COUNT = 5; // S·ªë l·∫ßn th·ª≠ t·ªëi ƒëa tr∆∞·ªõc khi ng·ª´ng t·ª± ƒë·ªông reset
const MAX_FALLBACK_RETRIES = 2; // S·ªë l·∫ßn th·ª≠ t·ªëi ƒëa cho model thay th·∫ø tr∆∞·ªõc khi b·ªè cu·ªôc

/**
 * Ki·ªÉm tra xem l·ªói c√≥ ph·∫£i l√† do rate limit kh√¥ng
 */
export function isRateLimitError(error: any): boolean {
  // Ki·ªÉm tra th√¥ng b√°o l·ªói c·ªßa Groq API cho rate limit
  if (typeof error === "string") {
    return error.includes("rate_limit_exceeded");
  }

  // Ki·ªÉm tra ƒë·ªëi t∆∞·ª£ng l·ªói
  if (
    error?.error?.code === "rate_limit_exceeded" ||
    (error?.message && error.message.includes("rate_limit_exceeded"))
  ) {
    return true;
  }

  // Ki·ªÉm tra response t·ª´ fetch API
  if (error?.status === 429) {
    return true;
  }

  return false;
}

/**
 * ƒê√°nh d·∫•u model ƒë√£ g·∫∑p rate limit v√† l·∫•y model thay th·∫ø
 */
export function handleModelRateLimit(currentModel: string): string {
  // ƒê√°nh d·∫•u model hi·ªán t·∫°i l√† b·ªã rate limit
  if (modelRateLimits[currentModel]) {
    modelRateLimits[currentModel].limited = true;
    modelRateLimits[currentModel].retryAt = Date.now() + RATE_LIMIT_RESET_TIME;
    modelRateLimits[currentModel].retryCount += 1;

    console.log(
      `[AI-MODEL] üî¥ Model ${currentModel} b·ªã rate limit. Chuy·ªÉn sang model thay th·∫ø.`
    );
  }

  // T·ª± ƒë·ªông reset rate limit sau m·ªôt th·ªùi gian n·∫øu s·ªë l·∫ßn th·ª≠ ch∆∞a v∆∞·ª£t qu√° ng∆∞·ª°ng
  if (modelRateLimits[currentModel]?.retryCount <= MAX_RETRY_COUNT) {
    setTimeout(() => {
      if (modelRateLimits[currentModel]) {
        modelRateLimits[currentModel].limited = false;
        console.log(
          `[AI-MODEL] üü¢ Reset rate limit cho model ${currentModel}.`
        );
      }
    }, RATE_LIMIT_RESET_TIME);
  }

  // Tr·∫£ v·ªÅ model thay th·∫ø
  return getAlternativeModel(currentModel);
}

/**
 * L·∫•y model thay th·∫ø cho model hi·ªán t·∫°i
 */
function getAlternativeModel(currentModel: string): string {
  // N·∫øu model hi·ªán t·∫°i l√† model ch√≠nh, tr·∫£ v·ªÅ model d·ª± ph√≤ng
  if (currentModel === MODELS.PRIMARY) {
    return MODELS.FALLBACK;
  }
  // Ng∆∞·ª£c l·∫°i, tr·∫£ v·ªÅ model ch√≠nh
  return MODELS.PRIMARY;
}

/**
 * L·∫•y model ph√π h·ª£p nh·∫•t ƒë·ªÉ s·ª≠ d·ª•ng t·∫°i th·ªùi ƒëi·ªÉm hi·ªán t·∫°i
 * ∆Øu ti√™n model PRIMARY n·∫øu kh√¥ng b·ªã rate limit
 */
export function getGroqModel(): string {
  // D·ªçn d·∫πp tr·∫°ng th√°i rate limit n·∫øu ƒë√£ h·∫øt th·ªùi gian
  Object.keys(modelRateLimits).forEach((model) => {
    if (
      modelRateLimits[model].limited &&
      Date.now() >= modelRateLimits[model].retryAt
    ) {
      modelRateLimits[model].limited = false;
      console.log(
        `[AI-MODEL] üü¢ T·ª± ƒë·ªông reset rate limit cho model ${model} do h·∫øt th·ªùi gian ch·ªù.`
      );
    }
  });

  // ∆Øu ti√™n model ch√≠nh n·∫øu kh√¥ng b·ªã rate limit
  if (!modelRateLimits[MODELS.PRIMARY].limited) {
    return MODELS.PRIMARY;
  }

  // N·∫øu model ch√≠nh b·ªã rate limit, ki·ªÉm tra model d·ª± ph√≤ng
  if (!modelRateLimits[MODELS.FALLBACK].limited) {
    return MODELS.FALLBACK;
  }

  // N·∫øu c·∫£ hai model ƒë·ªÅu b·ªã rate limit, tr·∫£ v·ªÅ model c√≥ th·ªùi gian retry s·ªõm h∆°n
  if (
    modelRateLimits[MODELS.PRIMARY].retryAt <=
    modelRateLimits[MODELS.FALLBACK].retryAt
  ) {
    return MODELS.PRIMARY;
  } else {
    return MODELS.FALLBACK;
  }
}

/**
 * H√†m ti·ªán √≠ch ƒë·ªÉ g·ªçi Groq API v·ªõi x·ª≠ l√Ω rate limit
 */
export async function callGroqAPI(
  messages: any[],
  options: {
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
  } = {}
): Promise<Response> {
  // L·∫•y model ph√π h·ª£p nh·∫•t hi·ªán t·∫°i
  let model = getGroqModel();
  const apiKey = process.env.GROQ_API_KEY;

  if (!apiKey) {
    throw new Error("GROQ_API_KEY is not defined");
  }

  // Log model ƒë∆∞·ª£c s·ª≠ d·ª•ng
  console.log(`[AI-MODEL] ü§ñ ƒêang s·ª≠ d·ª•ng model: ${model}`);

  try {
    // B·∫Øt ƒë·∫ßu th·ªùi gian request
    const startTime = Date.now();

    const response = await fetch(
      "https://api.groq.com/openai/v1/chat/completions",
      {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${apiKey}`,
        },
        body: JSON.stringify({
          model,
          messages,
          stream: options.stream ?? true,
          temperature: options.temperature ?? 0.7,
          max_tokens: options.max_tokens ?? 1024,
        }),
      }
    );

    // Th·ªùi gian ho√†n th√†nh request
    const requestTime = Date.now() - startTime;

    // N·∫øu API tr·∫£ v·ªÅ l·ªói
    if (!response.ok) {
      try {
        // Clone response tr∆∞·ªõc khi ƒë·ªçc body ƒë·ªÉ c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng
        const clonedResponse = response.clone();
        const responseText = await clonedResponse.text();
        let responseBody;

        try {
          responseBody = JSON.parse(responseText);
        } catch (e) {
          responseBody = { error: { message: responseText } };
        }

        // Ki·ªÉm tra xem c√≥ ph·∫£i l·ªói rate limit kh√¥ng
        if (
          response.status === 429 ||
          (responseBody?.error?.message &&
            responseBody.error.message.includes("rate_limit_exceeded"))
        ) {
          console.error(
            `[AI-MODEL] ‚ö†Ô∏è Rate limit reached for model: ${model}. Response: ${responseText}`
          );

          // X·ª≠ l√Ω rate limit v√† l·∫•y model thay th·∫ø
          const alternativeModel = handleModelRateLimit(model);
          console.log(
            `[AI-MODEL] üîÄ Switching to alternative model: ${alternativeModel}`
          );

          // B·∫Øt ƒë·∫ßu th·ªùi gian request thay th·∫ø
          const altStartTime = Date.now();

          // Th·ª≠ l·∫°i v·ªõi model thay th·∫ø
          let retryCount = 0;
          let altResponse = null;

          while (retryCount < MAX_FALLBACK_RETRIES && !altResponse) {
            try {
              const attemptResponse = await fetch(
                "https://api.groq.com/openai/v1/chat/completions",
                {
                  method: "POST",
                  headers: {
                    "Content-Type": "application/json",
                    Authorization: `Bearer ${apiKey}`,
                  },
                  body: JSON.stringify({
                    model: alternativeModel,
                    messages,
                    stream: options.stream ?? true,
                    temperature: options.temperature ?? 0.7,
                    max_tokens: options.max_tokens ?? 1024,
                  }),
                }
              );

              // Th·ªùi gian ho√†n th√†nh request thay th·∫ø
              const altRequestTime = Date.now() - altStartTime;

              if (attemptResponse.ok) {
                console.log(
                  `[AI-MODEL] ‚úÖ Request v·ªõi model thay th·∫ø ${alternativeModel} th√†nh c√¥ng sau ${altRequestTime}ms`
                );
                altResponse = attemptResponse;
                break;
              } else {
                console.error(
                  `[AI-MODEL] ‚ö†Ô∏è L·∫ßn th·ª≠ ${
                    retryCount + 1
                  }: Request v·ªõi model thay th·∫ø ${alternativeModel} th·∫•t b·∫°i sau ${altRequestTime}ms`
                );

                // ƒê·ªçc n·ªôi dung l·ªói t·ª´ ph·∫£n h·ªìi model thay th·∫ø
                const altErrorText = await attemptResponse.text();
                console.error(
                  `[AI-MODEL] L·ªói khi d√πng model thay th·∫ø: ${altErrorText}`
                );

                // TƒÉng s·ªë l·∫ßn th·ª≠ v√† ch·ªù m·ªôt ch√∫t tr∆∞·ªõc khi th·ª≠ l·∫°i
                retryCount++;
                if (retryCount < MAX_FALLBACK_RETRIES) {
                  console.log(
                    `[AI-MODEL] üîÑ ƒê·ª£i 1 gi√¢y v√† th·ª≠ l·∫°i v·ªõi model thay th·∫ø...`
                  );
                  await new Promise((resolve) => setTimeout(resolve, 1000));
                }
              }
            } catch (attemptError) {
              console.error(
                `[AI-MODEL] ‚ùå L·ªói k·∫øt n·ªëi khi th·ª≠ model thay th·∫ø l·∫ßn ${
                  retryCount + 1
                }:`,
                attemptError
              );
              retryCount++;
              if (retryCount < MAX_FALLBACK_RETRIES) {
                await new Promise((resolve) => setTimeout(resolve, 1000));
              }
            }
          }

          if (altResponse) {
            return altResponse;
          } else {
            // N·∫øu c·∫£ hai model ƒë·ªÅu l·ªói, tr·∫£ v·ªÅ th√¥ng b√°o l·ªói
            console.error(
              `[AI-MODEL] ‚ùå ƒê√£ th·ª≠ ${MAX_FALLBACK_RETRIES} l·∫ßn, kh√¥ng th·ªÉ s·ª≠ d·ª•ng model thay th·∫ø.`
            );

            // Tr·∫£ v·ªÅ response l·ªói c√≥ th·ªÉ x·ª≠ l√Ω
            return new Response(
              JSON.stringify({
                error: "C·∫£ hai model ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng, vui l√≤ng th·ª≠ l·∫°i sau.",
                suggestions: [],
              }),
              {
                status: 503,
                headers: {
                  "Content-Type": "application/json",
                },
              }
            );
          }
        }

        // N·∫øu kh√¥ng ph·∫£i l·ªói rate limit, throw l·ªói
        console.error(
          `[AI-MODEL] ‚ùå Error from Groq API (model: ${model}): ${responseText}`
        );
        throw new Error(`Error from Groq API: ${responseText}`);
      } catch (parseError) {
        console.error(
          `[AI-MODEL] ‚ùå Error parsing API response: ${parseError}`
        );
        throw parseError;
      }
    }

    console.log(
      `[AI-MODEL] ‚úÖ Request th√†nh c√¥ng v·ªõi model ${model} sau ${requestTime}ms`
    );
    return response;
  } catch (error) {
    console.error(
      `[AI-MODEL] ‚ùå Error calling Groq API (model: ${model}):`,
      error
    );
    throw error;
  }
}
